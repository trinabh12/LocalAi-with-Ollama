This is a great idea. A professional README is exactly what you need to back up that LinkedIn post and catch the eye of recruiters at companies like Microsoft or Amazon.

Since you are leading your own machine learning project at Hexaware Technologies, this project shows your ability to build the "connective tissue" between a raw AI model and a user-friendly interface.

---

# LocalAi Pro: Modern Flask Interface for Ollama

A sleek, dark-themed web interface for interacting with local Large Language Models. This project bridges the gap between the **Ollama** backend and a professional, responsive frontend.

## âœ¨ Features

* **Modern UI:** Glassmorphism design with a symmetric, responsive chat layout.
* **Dynamic Model Management:** Automatically fetches and lists all models installed on your local Ollama instance.
* **Real-time Switching:** Change models on the fly without restarting the server.
* **Status Indicators:** Visual feedback (Active/Switching) and "AI is thinking" indicators for a better user experience.
* **Local & Private:** Everything runs on your machine; no data leaves your local environment.

---

## ğŸš€ Step-by-Step Setup

### 1. Prerequisites

Ensure you have the following installed:

* **Python 3.8+**
* **Ollama:** [Download it here](https://ollama.com/)

### 2. Install Dependencies

Clone this repository and install the required Python packages:

```bash
pip install flask ollama

```

### 3. Project Structure

Ensure your files are organized as follows to comply with Flask's routing:

```text
/LocalAi-Chat
â”‚
â”œâ”€â”€ main.py          # Flask Server
â”œâ”€â”€ brain.py         # AI Logic (Ollama Integration)
â”œâ”€â”€ static/          # Assets
â”‚   â”œâ”€â”€ styles.css   # Modern Dark Theme
â”‚   â””â”€â”€ script.js    # Frontend Logic
â””â”€â”€ templates/       # HTML
    â””â”€â”€ chat.html    # The UI

```

### 4. Running the Application

1. Open Ollama on your computer.
2. In your terminal, navigate to the project folder and run:
```bash
python main.py

```


3. Open your browser and go to `http://127.0.0.1:5000`.

---

## ğŸ“¸ Screenshots

### **The Interface**

*Insert your high-resolution screenshot here (like the one we fixed earlier with the symmetric input bar).*

> **Tip:** You can use a tool like "Lightshot" or "Snipping Tool" to capture just the chat container.

### **Model Selection**

*Insert a screenshot showing the dropdown menu listing your `llama3.1:latest` or other models.*

---

## ğŸ› ï¸ Technical Implementation

* **Backend:** Flask (Python) handles API routing and state management.
* **AI Engine:** The `Brain` class interacts with the Ollama Python library to pull models and fetch chat responses.
* **Frontend:** Vanilla JavaScript with Fetch API for asynchronous communication and CSS Flexbox for the symmetric layout.

---

## ğŸ‘¨â€ğŸ’» Author

**Trinabh Mitra**

---
